{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward_sum:556.27\n",
      "reward_sum:196.71\n",
      "reward_sum:487.37\n"
     ]
    }
   ],
   "source": [
    "import torch                                    # 导入torch\n",
    "import torch.nn as nn                           # 导入torch.nn\n",
    "import torch.nn.functional as F                 # 导入torch.nn.functional\n",
    "import numpy as np                              # 导入numpy\n",
    "import gym                                      # 导入gym\n",
    "\n",
    "# 超参数\n",
    "EPSILON = 0.9                                   # greedy policy\n",
    "env = gym.make('CartPole-v0').unwrapped         # 使用gym库中的环境：CartPole，且打开封装(若想了解该环境，请自行百度)\n",
    "N_ACTIONS = env.action_space.n                  # 杆子动作个数 (2个)\n",
    "N_STATES = env.observation_space.shape[0]       # 杆子状态个数 (4个)\n",
    "\n",
    "# 定义Net类 (定义网络)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):                                                         \n",
    "        # nn.Module的子类函数必须在构造函数中执行父类的构造函数\n",
    "        super(Net, self).__init__()                                            \n",
    "\n",
    "        self.fc1 = nn.Linear(N_STATES, 50)                                      \n",
    "        self.fc1.weight.data.normal_(0, 0.1)\n",
    "        self.fc2 = nn.Linear(50, 50)                                      \n",
    "        self.fc2.weight.data.normal_(0, 0.1)\n",
    "        self.out = nn.Linear(50, N_ACTIONS)                                     \n",
    "        self.out.weight.data.normal_(0, 0.1)                                   \n",
    "    def forward(self, x):                                                       \n",
    "        x = F.relu(self.fc1(x))                                                 \n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "# 定义DQN类 (定义两个网络)\n",
    "class DQN(object):\n",
    "  def __init__(self):                                                         # 定义DQN的一系列属性\n",
    "    self.eval_net = torch.load('./dqn_eval_v3.pth')\n",
    "    self.eval_net.eval()\n",
    "\n",
    "  def choose_action(self, x):                                                 # 定义动作选择函数 (x为状态)\n",
    "    x = torch.unsqueeze(torch.FloatTensor(x), 0)                            # 将x转换成32-bit floating point形式，并在dim=0增加维数为1的维度\n",
    "    if np.random.uniform() < EPSILON:                                       # 生成一个在[0, 1)内的随机数，如果小于EPSILON，选择最优动作\n",
    "      actions_value = self.eval_net.forward(x)                            # 通过对评估网络输入状态x，前向传播获得动作值\n",
    "      action = torch.max(actions_value, 1)[1].data.numpy()                # 输出每一行最大值的索引，并转化为numpy ndarray形式\n",
    "      action = action[0]                                                  # 输出action的第一个数\n",
    "    else:                                                                   # 随机选择动作\n",
    "      action = np.random.randint(0, N_ACTIONS)                            # 这里action随机等于0或1 (N_ACTIONS = 2)\n",
    "    return action                                                           # 返回选择的动作 (0或1)\n",
    "\n",
    "\n",
    "dqn = DQN()                                                             # 令dqn=DQN类\n",
    "while True:\n",
    "  s = env.reset()                                                     # 重置环境\n",
    "  episode_reward_sum = 0                                              # 初始化该循环对应的episode的总奖励\n",
    "  while True:                                                         # 开始一个episode (每一个循环代表一步)\n",
    "#     env.render()                                                    # 显示实验动画\n",
    "    a = dqn.choose_action(s)                                        # 输入该步对应的状态s，选择动作\n",
    "    s_, r, done, info = env.step(a)                                 # 执行动作，获得反馈\n",
    "    x, x_dot, theta, theta_dot = s_\n",
    "    r1 = (env.x_threshold - abs(x)) / env.x_threshold - 0.8\n",
    "    r2 = (env.theta_threshold_radians - abs(theta)) / env.theta_threshold_radians - 0.5\n",
    "    new_r = r1 + r2\n",
    "    episode_reward_sum += new_r                           \n",
    "    s = s_                                                \n",
    "    if done:\n",
    "      print(f'reward_sum:{round(episode_reward_sum, 2)}')\n",
    "      break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supervampire",
   "language": "python",
   "name": "supervampire"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
